{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a80f8264-1235-4516-b74f-0fbb997d21a8",
   "metadata": {},
   "source": [
    "#Part One - Scrape the list of US presidents from https://en.wikipedia.org/wiki/List_of_presidents_of_the_United_States. \n",
    "using pandas and save them as a CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33cabd3e-368d-42f5-9130-1bd0a2b91994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2794e472-ff97-4cd6-b031-a1c6ffe72264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in /Users/cassidyjensen/.pyenv/versions/3.11.8/lib/python3.11/site-packages (5.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c301caa7-ee69-47b1-9752-aa85c40c87c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = pd.read_html('https://en.wikipedia.org/wiki/List_of_presidents_of_the_United_States')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "49294563-03c4-4f36-bbf9-800ffaedca8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32886bd2-13ba-43c4-a734-05d4cb4c86a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables[0].to_csv('presidents.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d348e4bc-99af-4ca2-966a-d0affacba6b3",
   "metadata": {},
   "source": [
    "PART TWO - Scrape the content of https://www.lemonde.fr/Links to an external site. and save it as a CSV.\n",
    "\n",
    "We want: titles, subhead, article URL, whether it's premium or not, byline, article type, image URL.\n",
    "\n",
    "You're going to be missing some article URLs - you should use try/except on them to avoid them being a problem, buuuut after that I would recommend printing the element to see why your approach isn't working (it's 100% possible to get all of the URLs!)\n",
    "If you want to make sure the element you're getting has a specific attribute, you can do something like .select_one(\"a[href]\"). That will only give you anchor tags (links) that have hrefs.\n",
    "Instead of yes/no for the premium question, you can think of it as \"put text in this column if the article is premium, don't put text in it if it is not\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3e26750b-d551-4868-98c0-516c00e795c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "response = requests.get(\"https://www.lemonde.fr/en/\")\n",
    "doc = BeautifulSoup(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "619c933b-da89-43b0-9240-b78a2dfc9163",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = doc.find_all(class_='article')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "7588e197-7197-4981-b53a-85f8e29a1a93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rows = []\n",
    "for item in items:\n",
    "    row = {}\n",
    "    row['title'] = item.find(class_='article__title').text\n",
    "    try:\n",
    "        row['url'] = item.find(class_=\"article article--nav\").get('href')\n",
    "    except:\n",
    "        row['url'] = item.get('href', None)\n",
    "    try:\n",
    "        row['premium'] = item.find(class_='icon__premium').text\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        row['image url'] = item.find(class_='lmd-link-clickarea__link').get('href')\n",
    "    except:\n",
    "        row['image url'] = item.get('href', None)\n",
    "    try:\n",
    "        row['subhed'] = item.find(class_=\"article__desc\").text\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        row['article__type'] = item.find(class_='article__type').text\n",
    "    except: \n",
    "        pass\n",
    "    try:\n",
    "        row['article__byline'] = item.find(class_='article__type').text\n",
    "    except: \n",
    "        pass\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "03fc649a-e6ca-42ad-ac45-b86685b2c5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.json_normalize(rows)\n",
    "df.head()\n",
    "df.to_csv(\"lemonde.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0417f1-0f72-4bd5-856f-0b47e3dc9d49",
   "metadata": {},
   "source": [
    "PART 3 -\n",
    "Scrape the list of third party drivers license locations from https://travel-id-documents.az.gov/authorized-third-party-driver-license-locationsLinks to an external site. but include the link. Save as a CSV. Since it's more than just the text from the table, this requires actually using BeautifulSoup :(\n",
    "\n",
    "Tips:\n",
    "\n",
    "You'll need user-agent headers for this one\n",
    "In class we always did something like item.find('h2') or the like because there was only ever one h2 we wanted. In this case there are four <td> tags that you want the text from! You'll need to use .find_all and treat them like normal lists (\"give me the text from the first one,\" + \"give me the text from the second one\" etc etc)\n",
    "It's okay to have columns you don't want or need! You can always remove them later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c88f632c-4ff0-4960-8abc-97321978fd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://travel-id-documents.az.gov/authorized-third-party-driver-license-locations\")\n",
    "doc = BeautifulSoup(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "9f55b384-ca58-4dc1-a9a9-640c8f0522e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = doc.find(class_='table')\n",
    "rows = table.select('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "fcc3b9be-e0b9-4ae9-94f1-7e3643076a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for row in rows[1:]:\n",
    "  result = []\n",
    "  if len(row.select('a')) > 0:\n",
    "    result.append(row.select('a')[0]['href'])\n",
    "  else:\n",
    "    result.append('')\n",
    "  cells = row.select('td')\n",
    "  for cell in cells:\n",
    "      result.append(cell.text.strip())\n",
    "  data.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "dec2896b-a285-4326-a7a2-597f40e8ab05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.DataFrame(data)\n",
    "df.columns = ['link', 'company', 'address', 'telephone', 'hours']\n",
    "df.to_csv(\"driverslicenselocations.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "e894843f-d82d-4622-a0c3-b39f5ec39164",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"driverslicenselocations.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b028c34-b0ad-45b4-b44e-649f48a8fcac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b72af2d-df4d-4daf-9e05-eb8a5c65e5a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
